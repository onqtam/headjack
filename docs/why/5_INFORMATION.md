


██████████████████████████████████████████████████████████████████
# SECTION: Openness & transparency
██████████████████████████████████████████████████████████████████

> "Given enough eyeballs, all bugs are shallow." - [Linus's law](https://en.wikipedia.org/wiki/Linus%27s_law), formulated by [Eric S. Raymond](https://en.wikipedia.org/wiki/Eric_S._Raymond)

> "The AI behavior guardrails that are set up with prompt engineering and filtering should be public — the creators should proudly stand behind their vision of what is best for society and how they crystallized it into commands and code. I suspect many are actually ashamed." - [John Carmack](https://twitter.com/ID_AA_Carmack/status/1760360183945965853)



██████████████████████████████████████████████████████████████████
# SECTION: provenance, deduplication & authenticity
██████████████████████████████████████████████████████████████████

> "In a world of infinite "content", only provenance will count" - [@punk6529](https://twitter.com/punk6529/status/1636275893545967616)

https://en.wikipedia.org/wiki/Data_lineage
> "To make sense of the information and arguments we read, we have to understand where that information comes from." - [Where Arguments Come From - The Consilience Project](https://consilienceproject.org/where-arguments-come-from/#:~:text=To%20make%20sense%20of%20the%20information%20and%20arguments%20we%20read%2C%20we%20have%20to%20understand%20where%20that%20information%20comes%20from.)

provenance graphs - imagine being able to check if any of the inputs of a story has been influenced by a specific source

If we can annotate facts and claims then we’d be able to track the provenance of ideas - or at least view their first occurance

we use symbols and categories in our minds and there's a single representation for any concept/entity/event - we should do the same for the global brain

> "So what do we do about this world we are living in where content can be created by machines and ascribed to us? I think we will need to sign everything to signify its validity." - [AVC](https://avc.mirror.xyz/JpTblGUpDMA7SMS6HYdoYgbQE9H_a_wYq330pit_aRU)

> "People generally only see the words emitted by a politician or corporation. They can't really see the actions. Voting records & balance sheets are currently less legible, less visible, less verifiable, than tweets. Cryptography could fix this..." - [@balajis](https://twitter.com/balajis/status/1596412557127954433)

apps should be able to show you all references to a document - perhaps they might show specific UIs for those that are supported and all others should be in the "other" list without visualization

deepfake Lexman Artificial & Eric Weinstein
https://twitter.com/lexman_ai/status/1615861599197171713

we will share the digital space with AIs and a web of trust on top of PKI will be incredibly important

> "We cannot reasonably enforce that AI generated content is watermarked, but we can build a robust, reliable, cheap and scalable trust chain infrastructure for certifying content created and vouched for by humans." - [Joscha Bach](https://twitter.com/Plinz/status/1710409853565628463)

██████████████████████████████████████████████████████████████████
# SECTION: Language & its limits
██████████████████████████████████████████████████████████████████

> "The more I think about language, the more it amazes me that people ever understand each other at all." - [Kurt Gödel](https://www.goodreads.com/quotes/336516-the-more-i-think-about-language-the-more-it-amazes)

remove bannon quote - or at least his name

> "In my view, aiming at simplicity and lucidity is a moral duty of all intellectuals: lack of clarity is a sin, and pretentiousness is a crime." - [Karl Popper](https://www.goodreads.com/quotes/7444490-in-my-view-aiming-at-simplicity-and-lucidity-is-a)

Steve Bannon's infamous quote [`"to flood the zone with $#it"`](https://edition.cnn.com/2021/11/16/media/steve-bannon-reliable-sources/index.html) perhaps best describes today's information landscape. Language is unbelievably complex and contains a lot of implied & assumed meaning. Almost nothing is properly interlinked and there's a crisis of fragmentation, duplication & lack of provenance. Data is hard to process, aggregate & classify unambiguously and we have reached the [spamgularity](https://twitter.com/DanielleFong/status/1622315719298883584) with the recent advancements in LLMs:

> "But if thought corrupts language, language can also corrupt thought." - [George Orwell](https://www.goodreads.com/quotes/72144-but-if-thought-corrupts-language-language-can-also-corrupt-thought)

> "Speech as proof of work is dead." - [Gordon Brander](https://twitter.com/gordonbrander/status/1622999277411741698)

When all you have is language, it becomes the hammer and everything else a nail. Furthermore:

> "What might take minutes to consume might also take hours to analyze. By looking deeply into the underlying logic of text, students can see how a complete perspective can be implied in a partial sentence, how a conclusion may be claimed with no premise nor evidence, and in general comprehend how abbreviated our logical reasoning is on a daily basis. But the technical density of extricating 178 definitive claims from 16 minutes of semi-scripted speech is still just an introductory exercise.
>
> It’s one thing to look deeply into “meaning” that is implied with language, but it’s another to look broadly and come to understand the contextual density of language. By that I mean the in-group cultural references, code-words, assumptive glazing over details, and doublespeak that hint about the world-views, insinuations, and even insults that are present, but are not explicit. It’s in the flippant addition of an adjective or a sly inflection of speech. These are the subtle signals that some audiences pick up on and others are completely oblivious to. This density hides behind language, and can cloak subjectively interpretable, unarticulated, and therefore a nearly undetectable broader context." - [The Society Library](https://misinfocon.com/missing-context-in-the-internet-congress-42eb319aed1e#:~:text=what%20might%20take,undetectable%20broader%20context.)

██████████████████████████████████████████████████████████████████
# SECTION: Integration
██████████████████████████████████████████████████████████████████


There is no integration without complete deduplication of data and identities

██████████████████████████████████████████████████████████████████
<!-- # The need for structured data -->
# SECTION: Structured data & machine readability
<!-- # Raising the signal/noise ratio -->
<!-- # Structured data & improving the signal/noise ratio -->
██████████████████████████████████████████████████████████████████

Information theory could be as important as or even fundamental to matter & the laws of physics - they are tightly interwoven. Nature strives to remove unnecessary bits when it comes to exchange of information - and so should we.

some people think that with LLMs we've reached the pinnacle of communication with machines with something as natural as text but that couldn't be further from the truth - we need structured data more than ever

What if:

- most comments were turned into reactions/votes/assertions that are unambiguous? There's always the option to comment on your reaction/vote/assertion to clarify it and add context.
- we could specify importance levels & severity thresholds to each piece of communication?
- Twitter extensions didn't clutter with text tweets like `@SaveToNotion`, `@readwise save`, `@threadreaderapp unroll` & [`@memdotai mem it`](https://twitter.com/MaficoNFT/status/1591510117094428678) but could use dedicated message types that can be filtered & treated in a special way?
<!-- - #hashtags weren't part of the text but utilized a tag system? -->



== Upsides in moving from language to structured data:
== If we moved the vast majority of online acitivity to structured data away from language we'd get:

- less need for translation & localization
- less need for content moderation
- less emotionally manipulative language & framing such as the [Russell conjugation](https://en.wikipedia.org/wiki/Emotive_conjugation)
    In rhetoric, emotive or emotional conjugation (also known as Russell's conjugation)[1] is a rhetorical technique used to create an intrinsic bias towards or against a piece of information. Bias is created by using the emotional connotation of a word to prime a response from the audience by creating a loaded statement.

    https://twitter.com/AshleyRindsberg/status/1666052378083352578
- less need to think of the right words - reducing friction in expression


[Lakoff framing](https://en.wikipedia.org/wiki/Metaphorical_framing)

> "The researchers found that, on the one hand, participants who read the "crime is a beast" metaphor were more likely to support crime-fighting strategies such as increasing the number of police officers and building more jails. On the other hand, participants who read the "crime is a virus" metaphor were more likely to support crime-prevention strategies, such as implementing social reforms and calling for an investigation into the root causes of crime." - [source](https://en.wikipedia.org/wiki/Metaphorical_framing#:~:text=The%20researchers%20found,causes%20of%20crime.)

> "Framing is about getting language that fits your worldview. It is not just language. The ideas are primary and the language carries those ideas, evokes those ideas." - [George Lakoff](https://medium.com/@ennuid/george-lakoffs-framing-101-7b88e9c91dac#:~:text=Framing%20is%20about%20getting%20language%20that%20fits%20your%20worldview.%20It%20is%20not%20just%20language.%20The%20ideas%20are%20primary%20and%20the%20language%20carries%20those%20ideas%2C%20evokes%20those%20ideas.)

Russell conjugation - horse dewormer vs ivermectin

How much context and nuance has been lost in translation?

We need not only fact checking but also frame checking - around language

We have reached “X destroys Y” in headlines through virality A/B testing - what if we could add a negative weight to such content?


unlike text, structured data is easily aggregatable for the purpose of visualization through graphs & charts which makes it easily comprehensible

Let's take 2 sides of a debate around vaccines - Brett Weinstein & Scott Adams. It is impossible for someone to quickly see where they've been on the debate throughout time without pouring hundreds of hours sifting through old tweets & watching endless podcasts & episodes. Their opinions could have been codified with structured data & logged as time-series so that we can compare, chart & plot the evolution of their views. Structured data should be incentivized.



> "Larger vocabulary means better data compression – fewer words needed to express concepts – so higher data rate" - [Elon Musk](https://twitter.com/elonmusk/status/1636421504538738689)

serialization of thought into language is lossy


currently we channel our energy in a useless way - the best we can do is get a hashtag trending. But with this system every type of interaction would count

so much expressed opinions & views are left unanalyzed & unaggregated because they're in text - they have an ephemeral impact for a moment and then fade

by increasing the amount of structured data we'd lower the amount of text that has to be checked for the rules - such as incitement of violence. Many freeform posts can be replaced by structured data

reducing information entropy - the level of surprise
but aren't we actually reducing the ambiguity?
bringing order
reducing the symbol space

reducing the vocabulary of common knowledge



██████████████████████████████████████████████████████████████████
<!-- # The long-form media source: thought-graphs -->
<!-- # Thought-graphs: the long-form media source -->
# Thought-graphs as the source of long-form
<!-- # Thought-graphs: an upgrade to linguistics -->
██████████████████████████████████████████████████████████████████

“Semantic graphs” instead of thought
Semantic graphs as the primary source


Perhaps thought-graphs are overkill. Maybe only simple fact/assertion statements should be present

Even thought-graphs could be generated from text as an initial pass, although that would need a human to take a look

People can still consume text and audio but they’d have the option to trace and inspect the inputs & how the saussage is made

We can reality-test our beliefs

<!-- What about complex thoughts? Again, language is a poor medium - it evolved hundreds of thousands of years ago but now we can do a lot better.

knowledge engineering


> "People can’t share knowledge if they don’t speak a common language." - [Thomas Davenport, 1997](https://team12pori.wordpress.com/ontology-information-science/#:~:text=%E2%80%9CPeople%20can%E2%80%99t%20share%20knowledge%20if%20they%20don%E2%80%99t%20speak%20a%20common%20language.%E2%80%9D)
<IMG TOWER OF BABEL>
https://upload.wikimedia.org/wikipedia/commons/thumb/f/fc/Pieter_Bruegel_the_Elder_-_The_Tower_of_Babel_%28Vienna%29_-_Google_Art_Project_-_edited.jpg/1280px-Pieter_Bruegel_the_Elder_-_The_Tower_of_Babel_%28Vienna%29_-_Google_Art_Project_-_edited.jpg
https://commons.wikimedia.org/wiki/File:Pieter_Bruegel_the_Elder_-_The_Tower_of_Babel_(Vienna)_-_Google_Art_Project.jpg

...to speak a common Language:
● common symbols and concepts (Syntax)
● agreement about their meaning (Semantics)
● classification of concepts (Taxonomy)
● associations and relations of concepts (Thesauri)
● rules and knowledge about which relations are
allowed and make sense (Ontologies)

> "Our highest-bandwidth information channel is not speech. It's vision and touch." - [Yann LeCun](https://twitter.com/ylecun/status/1640595183946407936)

instead of crowd-sourcing the truthfulness of a text post we can crowd-source only the mapping of that post to semantic triplets of assertions and automatically check those against an external set

Knowledge Engineering with Semantic Web Technologies by Dr. Harald Sack
https://www.youtube.com/playlist?list=PLoOmvuyo5UAcBXlhTti7kzetSsi1PpJGR

we can deduce implications of what we've laid out semantically
We can then compare those implications around a different set of facts/assertions - fact-checking our thoughts - sufracing bias & contradictions. What if we laid out all our beliefs (privately) as a database such that we can later fact-check our new thoughts against that set of beliefs?

TODO: Reification - giving an address to any RDF-like triplet statement/assertion
https://en.wikipedia.org/wiki/Reification_(knowledge_representation)

We'll be constructing our (SPARQL?) queries with natural language by interfacing with LLMs like ChatGPT.

SPARQL
Prolog
https://en.wikipedia.org/wiki/Cypher_(query_language)

knowledge engineering & semantic representation have been used only for knowledge and facts - we've yet to use it to pattern-match & bridge opinions at scale

the elite form of communication will become semantic

Writing forces us to think, but the act of serializing a complex web of interconnected thoughts is very lossy - most connections, weights & context are either lost or conveyed differently, and every attempt to fix this leads to overly verbose, boring and incomprehensible text. -->

What about complex thoughts? The act of serializing a complex web of interconnected thoughts with language is very lossy - most connections, weights & context are either lost or conveyed differently, and most attempts to fix this lead to overly verbose, boring and incomprehensible text. Language is a poor medium - it evolved hundreds of thousands of years ago but now we can do a lot better.

What if we expressed our thoughts as directed graphs of ideas, entities & concepts with weights, probabilities & causal relationships? Our canvas could be a node-based software that's seamlessly integrated with the semantic web & the Giant Global Graph instead of a limited text editor.

<!-- > "Writing is the process by which you realize that you do not understand what you are talking about. Importantly, writing is also the process by which you figure it out." - [Why Write? - by Shane Parrish](https://fs.blog/why-write/#:~:text=Writing%20is%20the%20process%20by%20which%20you%20realize%20that%20you%20do%20not%20understand%20what%20you%20are%20talking%20about.%20Importantly%2C%20writing%20is%20also%20the%20process%20by%20which%20you%20figure%20it%20out.) -->

> "If you’re thinking without writing, you only think you’re thinking." - [Leslie Lamport](https://fs.blog/how-to-think/#:~:text=%E2%80%9CIf%20you%E2%80%99re%20thinking,%E2%80%94%20Leslie%20Lamport)

Writing text forces us to think, but graph construction & exploration would make our thinking more precise and our understanding deeper. Mind Maps are superior when it comes to comprehension and retention of information because they organize it spatially & hierarchically - engaging our brains visually. The source should always be a graph because our that's how our minds represent knowledge & ideas - even though we communicate serially.

Let's take the [`"Why I think there's a one-in-six chance of an imminent global nuclear war (October 2022)"`](https://www.lesswrong.com/posts/Dod9AWz8Rp4Svdpof/why-i-think-there-s-a-one-in-six-chance-of-an-imminent) post by Max Tegmark for example and <u>**ignore if we agree with him or not**</u>.

TODO: this is a bad example - the picture ain't good and his arguments arent good either and ppl will nitpick them

<a href="https://www.lesswrong.com/posts/Dod9AWz8Rp4Svdpof/why-i-think-there-s-a-one-in-six-chance-of-an-imminent">
<img src="/img/Why_I_think_there's_a_one-in-six_chance_of_an_imminent_global_nuclear_war.jpeg"/>
</a>

His thesis is effectively a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain), which he goes on to explain in greater detail with text. But his tool to make that graph was probably for simple diagrams - incomparable in terms of capabilities to what we're envisioning here. Graph-first thinking just isn't how we express & communicate (yet).

But why would we do that? What are the benefits?

1. We could **render** our thoughts into different mediums: text, audio & video through the use of LLMs & generative AI - expanding the accessibility of our work beyond those that can view thought-graphs. A graph has many ways to be traversed so we can ask the AI to generate multiple versions. We could also specify a node path (which nodes/subgraphs and in what order) for the serialization into text and also paragraphs to be inserted at specific points (perhaps you want to write the introduction or conclusion yourself). We could also have fine-grained control such as annotating specific nodes to be skipped from the serialization or to signal their importance so that the AI adds more or less emphasis. We could have movie-maker-like software with a timeline where we orchestrate the traversal. This wasn't possible until the recent advancements of LLMs & AI.

1. Imagine being able to say "**view source**" of a text article and being shown the thought-graph that the author used to generate it. The serialized output could also have an automatically generated mapping so that we can see which regions correspond to which nodes & links with some highlighting and we could have both displayed side by side for synchronized "playback".

1. What about **editing**? Refactoring & "rewriting" will be just a matter of moving & reconnecting nodes & subgraphs, followed by a re-render of the outputs by the AI.

1. We could generate **change reports** & summaries for a thought-graph throughout time.

1. We could **debug / fact-check** the logic in the circuits of thought-graphs. Choose ontologies & fact databases and evaluate the connections of a graph for logical consistency against them.

1. We could **pattern-match** our thought-graphs with others and we could compute the similarity of (sub)graphs and track the provenance & plagiarism of ideas.

1. **Discoverability** - the better you annotate your thought-graph with well-known & widely used semantic web concepts & ontologies - the easier it will be to discover your content. People don't put #hashtags without a good reason (even though they are extremely limited).

1. The [NYT](https://www.nytimes.com/) & [FT](https://www.ft.com/) have their own proprietary knowledge graphs but they publish only plain text. **Analysis** of journalism is at the level of word use frequency throughout time.

    > "It's clearly visible in [@DavidRozado](https://twitter.com/DavidRozado)'s word frequency graphs. They change a lot faster than usage. Injustice, for example, was not invented in 2013." - [Paul Graham](https://twitter.com/paulg/status/1272593601932013568)

    > "According to a new study by David Rozado, there has been a big increase in news headlines suggesting fear, anger, disgust, and sadness since 2000, and especially since about 2010. Journalists are pushing your buttons." - [Paul Graham](https://twitter.com/paulg/status/1583058588687212544)

    more charts
    https://twitter.com/paulg/status/1136962504343662592

    https://davidrozado.substack.com/p/ppdwnmd

    https://twitter.com/TheRabbitHole84/status/1655968201422012418

    > "We should discuss this graph more. (peer review)" - [Eric Weinstein](https://twitter.com/EricRWeinstein/status/1483860161965932544)

    TODO: https://davidrozado.substack.com/p/gag

    Journalism goes hand in hand with frame control ([Russell conjugation](https://en.wikipedia.org/wiki/Emotive_conjugation) - **what is** vs **how to feel about what is**), [Gell-Mann Amnesia](https://en.wikipedia.org/wiki/Michael_Crichton#GellMannAmnesiaEffect), cherry picking and many other problems. Imagine the introspection possible if everything was expressed as thought-graphs and was interlinked. What if we could fork & edit thought-graphs to remove bias? Can we become post-narrative?

    “A rocket will never be able to leave Earth's atmosphere.” -New York Times, 1936
    https://twitter.com/engineers_feed/status/1698186317606134162

    Michael Crichton > Quotes > Quotable Quote
    https://www.goodreads.com/quotes/65213-briefly-stated-the-gell-mann-amnesia-effect-is-as-follows-you

1. If no thought-graph exists for an article - anyone could construct it and tag it to the original resource - making it discoverable for anyone to use. <!-- TODO:LINK TO THE OTHER SECTION -->

But could such a change happen? A good example are [Jupyter](https://en.wikipedia.org/wiki/Project_Jupyter) notebooks whose documents are interactive with parts of them being executable code, along with the option to plot/visualize/reuse the results of that code. We will continue reinventing how we interface with technology and information - static plain text is not the be-all end-all.

> "In 2021, Nature named [Jupyter](https://en.wikipedia.org/wiki/Project_Jupyter) as one of ten computing projects that transformed science." - [Project Jupyter, Wikipedia](https://en.wikipedia.org/wiki/Project_Jupyter#:~:text=In%202021%2C%20Nature%20named%20Jupyter%20as%20one%20of%20ten%20computing%20projects%20that%20transformed%20science.)

<!-- imagine laying out the case for banning TikTok as a thought-graph or a debate map
TODO: debate map section here? -->

██████████████████████████████████████████████████████████████████
# Second brain
██████████████████████████████████████████████████████████████████

this is relevant to the above section

██████████████████████████████████████████████████████████████████
<!-- # Query for anything related to something -->
<!-- # Anything that's relevant -->
<!-- # Everything that's relevant -->
<!-- # Anything relevant to something -->
<!-- # Free-form associations: anything-to-anything -->
# Show everything related - "Pull that up Jamie"
██████████████████████████████████████████████████████████████████

suppose a story is written by you - wouldn't you want to fact-check it when it is concerning you?

Pull that up jamie - common knowledge is easier this way - also when networks arent fragmented

What if we could see the TL;DR for anything? (connected to changing the titles for less clickbait)



We should be able to see all references & content related to any identity, event/document or entity/concept/noun as a collection of comments, reactions/votes, annotations, redirections and edits/forks which we could organize & filter through the UI. Examples:

1. Some YouTube videos have comments disabled. Why? Could be many reasons: **1)** perhaps they don't want to have their content associated with low quality comments, **2)** maybe they don't like the [scammers & bots](https://prnt.sc/ORguwjXfLgLR) and want to protect their audience, **3)** maybe they haven't fully thought this through, **4)** or maybe they are actively trying to hide something. But anything public can be discussed on other platforms (Reddit?) - all you've done is made it harder to find that discussion but it's still there (and is most likely fractured across different platforms/subreddits). The valid reasons (**1** and **2**) can be negated with better tooling for content moderation - both for active participants (tagging, annotating, voting) & the passive ones (simply consuming & filtering through the UI).

1. What if we could provide alternative titles for content and crowdsource the ones which most accurately describe it - countering the clickbait? Users could choose to view the original titles or the crowdsourced versions if there's consensus for a specific title to be way better than the original. This is already happening in StackOverflow - titles of other people's posts can be edited by moderators (or anyone?) with the goal of improving accuracy. What constitutes "better" and which/how votes are counted can be configurable too. But wouldn't content creators object?! Sure, but what are they going to do? But data can be composed in any way imaginable in the Giant Global Graph. This is like taking the best comment about what a post/video should be titled and through something like a browser extension programatically using that instead of the original in the UI - you can't stop that. We could even calculate clickbait scores for content creators - I bet many would have more than 90% of their content re-titled according to some criteria. We could even feed such scores into recommendation algorithms - **disincentivizing clickbait**.

TODO:
Deceiving others to watch your content just for your own good is narcissistic - are we seeking knowledge or do we want to feed and grow narcissism?

1. The perpetual AMA (Ask Me Anything) - currently only a small handful of people have done [AMAs in Reddit](https://www.reddit.com/r/AMA) that last only up to a few days with an extremely limiting interface in a sporadic/random fashion. What if every account had an implicit AMA associated with them (something like [ask.fm](https://ask.fm) but a lot more powerful) and we could ask any leader/CEO/celebrity & hyper agent questions with special message types and link them to their accounts? We could collectively surface the most pressing ones and make it glaringly obvious if the important ones aren't answered or if the answer is of poor quality - through different dashboards/charts & sorting/filtering. We could hold them to higher standards & more scrutiny. What if we could pressure companies & CEOs about the use of Cobalt from African mines with slave labor? Doing a poll on [change.org](https://change.org) is extremely ineffectual, but if something like it was integrated with a new more expressive type of social media for virality - it could have drastically different results. It is easy to ingore some obscure app but not when everything is built on top of the same interconnected identity & data network.

1. Reactions that relate to a piece of content. Let's take the [`"Jillian Michaels: Don't believe the keto diet hype | Big Think"`](https://www.youtube.com/watch?v=LOPOcBVzm7s) video for example. How do we determine if what's in there true? All we have is a comment section and a sidebar with algorithmic recommendations. Turns out there are [a number of reactions](https://www.youtube.com/results?search_query=jillian+michaels+keto+reaction) but they're not shown next to the video in any way - how would we know about their existence if we didn't explicitly search for them? The goal is not to be exposed to multiple points of view & reach some conclusion - it's only engagement. The videos are not interlinked, but suppose they were - we could have a list of reactions displayed along with scores of disagreement and other factors and we could more easily discern which one to watch next. <!-- The way to expose the complete failure & corruption of the FDA & their misguided policies is through a bottom-up crowdsourced effort enabled by the Internet and further enabled by linked data, identities & reputation. -->

https://twitter.com/goddeketal/status/1728136904028365161

1. Take the YouTube playlist [`"Journey into information theory | Computer Science | Khan Academy"`](https://www.youtube.com/playlist?list=PLSQl0a2vh4HC9lvrBhVt4UUkhzpp3N5_x) for example. The videos are out of order, but someone has made another playlist [`"Journey into information theory (in order)"`](https://www.youtube.com/playlist?list=PLq2EDMGUqkHHjHLhp88RY-e5mN7uGMP75) that is fixed. This is essentially a fork - one that is better than the original. Why can't we see a crowdsourced redirection suggestion attached to the original?

A unified identity layer, the ability to deduplicate content, to annotate semantically & the move to structured data greatly helps with being able to pull everything related to something. However, one concern is negative reputation & "scarlet letters" - the ability to taint an entity and have the bad reputation follow it without the ability to hide it. But we can develop tools, norms & criteria to evaluate/rank/filter any document & reaction that relates to something. Moreover, even if there isn't a direct data connection between 2 entities (one referring to the other), as long as they are both public someone could index them and make that link. This problem already exists today.

<!-- hiding inconvenient facts is most worthwhile to those with the most power. Negative reputation for any entity is a net positive despite the drawbacks -->


<!--
the ability to be able to sever the connection between an action of yours X to criticisms of it has enabled corruption to bubble up

["Sunlight is the best disinfectant"](https://en.wiktionary.org/wiki/sunlight_is_the_best_disinfectant)

> "Darkness cannot drive out darkness; only light can do that." - [Martin Luther King Jr.](https://www.azquotes.com/quote/158968)


when we're reading books, we should be able to check all feedback and activity around each reference - what has been crowdsourced. Imagine being able to colorize asserts that reference external sources based on a score showing if there's consensus this is true or there's a dispute. We can have a heatmap of the entire book/article to judge how trustworthy it is according to our criteria and which votes count
-->


██████████████████████████████████████████████████████████████████
# wikipedia
██████████████████████████████████████████████████████████████████

competing wikipedia-like pages written by different people & being able to select between them? no more "trusted source" issues?

Wikipedia and different prisms
Slicing and dicing - chapter name

“POV Issues” with the 1911 Encyclopedia Brittanica according to 2023 Wikipedia
> "The point of view held by the 1911 Encyclopædia Britannica is roughly the one of the British and American educated classes at the beginning of the twentieth century."
https://twitter.com/mgellison/status/1635897627203211264

we can use something like ChatGTP to summarize the results of polls from different points of view and write the text for a topic and always keep it up to date. This is the decentralized wikipedia.

imagine being able to pick your "wikipedia editors" - people who curate information in the open web while building a track record - and then anyone would be able to filter information differently based on their own personal criteria - multiple times from different angles (perhaps also by adopting someone else's criteria?)


██████████████████████████████████████████████████████████████████
# querying & palantir foundry
██████████████████████████████████████████████████████████████████

LLMs !!!

what if we had an LLM that we could ask to find the right filters, inputs and weights to find the best of X, and they would construct us a search preset that is best suited for our needs.

BigQuery 2.0

Graph tool for constructing queries - data processing pipelines like in Foundry

everyone can have access to something like Palantir Foundry for the entirety of the open web

localized queries - "looking for blood donor"?

THIS also relates to provenance
data pipelines & lineage - anyone would be able to do such complex analysis on public data
https://en.wikipedia.org/wiki/Data_lineage
ontology
Foundry 2022 Operating System Demo
https://www.youtube.com/watch?v=uF-GSj-Exms
reproducibility

a spreadsheet-like view over graph data - what if you could define owners of each cell so that they can update their elements and have that in a shared spreadsheet? And why not fork the sheet?

the decision maker: build trees of predicates and based on truth for each of them you can determine the outcome. Be able to play with such a system

you stumble upon a statement by an account and you also see a few of their other statements and completely disagree with them. What if you could run a query over all of their statements such as "how much overlap does their worldview have with mine"?

What if everyone could have such a computed score for every other account?

imagine being able to view a multi-level depth tree with all citations and conversations and conversations around those conversations and being able to filter the "outcome" based on criteria - labels/annotations/reactions and who we trust for placing those labels/annotations/reactions

We should all want informational freedom when it comes to querying public information

██████████████████████████████████████████████████████████████████
# Curators & labelers
██████████████████████████████████████████████████████████████████

This is how we should think about humans in the giang global graph:
https://www.youtube.com/watch?v=V0XfleKJSXM
History of Ethereum 2013-2018 (Git Visualization)
gource - git visualization

"reshaping the influencer"

We can let humans rank resources for various topics

What if we could flag [Bad Faith Communication](https://consilienceproject.org/the-endgames-of-bad-faith-communication/#accordion-1:~:text=and%20education%20crises.-,Common%20Strategies%20of%20Bad%20Faith%20Communication,-Misleading%20with%20facts)?

we should all be labellers - in an open way such that anyone can harness our collective intelligence

What if the way an event or a piece of news to get to more eyes was not through recommendation algorithms but through explicit annotation of what's inside - such that people can have explicit interest in those? What if people would rank stories by importance and then subscribe to filters based on people they trust would rank stories properly? What if you could allocate an attention budget of say 30 minutes per day or 20 news items and get the top after sorting, with the option to get the next batch if you want more?

Four Ways of Understanding Facts
https://consilienceproject.org/how-to-mislead-the-facts/#:~:text=Four%20Ways%20of%20Understanding%20Facts

the liquid democracy stuff can be used here to "vote" on who is a good curator for different topics

what about flagging an HR person as spammy? that should have its own specialized message type

we can crowdsource X and gamify it into success - it's been done before with initiatives like foldit @home
reputation! badges!

badges:
https://twitter.com/TitterTakeover/status/1622323886611202051

what if we could tag/annotate content where human faces are using filters - or at least that we think its happening?
what if we could choose that the recommender AIs do not show us faces with artificial filters (annotated as such by others) - wouldn't that disincentivize the use of such filters?
Obviously this isn't thought through completely - it could turn out bad if the annotation isn't reliable - its just trying to illustrate a point.
we could use AI to detect this instead of human annotation & reactions
we can have a lot more choice in what is being promoted in media


██████████████████████████████████████████████████████████████████
# organizing information - interlinking & deduplicating data
██████████████████████████████████████████████████████████████████

== problems:
- 
== solutions:
- 
- 
- knowledge graphs & the semantic web

google have failed with their mission - they have not organized the world's information - they have merely ranked it - organization is much broader
Google failed - they didnt organize the worlds information - they merely ranked it

just like in open source problems need to be solved only once, the same way in a world of unified identity & deduplicated concepts interwoven with social media debates need to be had only once

stable identity keys allows for stable concept/collection/noun keys which allows for stable mapping and deduplication of concepts

We need to deduplicate & reduce the amount of unique joinable keys for nouns, concepts, events & products. In the case when the same noun is published multiple times by different entities - maps of equivalence could be published so that nouns are equated to mean the same thing. We should be able to toggle which equivalence maps to enable.

joinability

The relational model is built on the premise of stable identifiers as foreign keys.

key stability ==> information deduplication & integration

If you choose an obscure tag or even if its the same but from an unpopular collection you'd simply limit your visibility & discoverability - there's an implied incentive to use the most popular collections when tagging your content


https://twitter.com/SocietyLibrary
the internet is unstructured
linked knowledge database
libraries & debate maps
libraries - concepts can be linked to different multimedia expressions - resources that describe it
debate maps are the ontological structure of the debate - aimed at deliberation

what if wikipedia-like "pages" were sitting on top of debate maps, highlighting an editorialized take and perhaps generated/summarized by an LLM? And what if people could choose the editorialization?

giant global graph
https://en.wikipedia.org/wiki/Giant_Global_Graph
a massively multiplayer interlinked knowledge database

machine-understandable knowledge representations
subject-predicate-object triplets in the semantic web
these are just like subject-verb-object of the human natural language - there's an equivalence

sensemaking is about creating maps

redundant knowledge & broken chains of provenance

interlinking enables discoverability - you'd want to reference something specific because then you could be discovered from the referenced thing with a "references" view

we should be able to annotate parts of movies that we are crituiquing and be able to see all public critiques when viewing the original media


DIKW pyramid
data, information, knowledge, and wisdom
https://en.wikipedia.org/wiki/DIKW_pyramid

“Data transforms to information by convention, information to knowledge by cognition, and knowledge to wisdom by contemplation.”
https://www.uio.no/studier/emner/matnat/ifi/nedlagte-emner/INF5020/h04/undervisningsmateriale/HANDOUTS/L2a-data-info-knowledge.pdf

"parental advisory explicit semantics" - image from the web


uploading a clip that could have easily just referenced a part of the whole shouldn't be encouraged - perhaps you'd need to pay for uploading it?

██████████████████████████████████████████████████████████████████
# stable documents
██████████████████████████████████████████████████████████████████

IPFS CIDs also let you do [Xanadu](https://en.wikipedia.org/wiki/Project_Xanadu)-style transclusion with standoff markup. Offsets don't break because the CIDs are immutable.
https://twitter.com/gordonbrander/status/1619370073722089472

highlighted parts of a page - with surrounding characters - for stable linking of documents even if there have been changes to them elsewhere in a new URI

Suppose 2 public intellectuals start debating something - others should be able to annotate that conversation with the right tags and topics and add structure to it

overlaying annotations for pieces that lack it


# Intra-document addressing

In Medium you can tweet a selection (sentence/word/paragraph) but when going back to the article from the tweet you don't get shown the original selection. With some archival services you can point to a text selection - for example [this link has `"Prussian Model"`](https://archive.ph/O2D45#selection-635.4-635.18) selected from the title when you open the page (`#selection-635.4-635.18`) and you can change the selection which also changes the URL, but that's possible only because there's a specific hash in the URL (`O2D45`) and the document is guaranteed not to change in the archive - however that's not the case with Medium where the authenticity of documents is host-certified and they can change in time.

With Headjack URIs point to a specific version of a document and as explained in the [addressing chapter](names_and_paths.md#addressing-within-content) we could point to parts of documents in the URIs. If a document has been changed, updates will have their own new URIs and when an application is showing an old URI with intra-document addressing it could:
- either show a label that there's a newer version of the document and the user can switch
- or directly show the new version if it's possible to transfer the selection without conflicts

Headjack's intra-document addressing is universal - it works for audio & video too and the application from the [startup case study](startup_case_study.md) could display this clip with this quote in a much better way:
> "The Internet creates 1 giant aggregator for everything." - [@naval](https://youtube.com/clip/UgkxphJhihcVY-U-PLFEvDl1m7Rb-iq4CGgo)

This can be pushed further - any composition/remix/meme of media could contain the references to the original text/pictures/audio/video so the sources of something can be traced and credited - imagine something like a movie maker that composes from other clips and all metadata is retained.

TODO:

https://subconscious.substack.com/i/49124972/text-fragments-select-excerpts-by-search
https://wicg.github.io/scroll-to-text-fragment/
https://support.google.com/chrome/answer/10256233?hl=en&co=GENIE.Platform%3DDesktop=
https://en.wikipedia.org/wiki/Vannevar_Bush#:~:text=wholly%20new%20forms%20of%20encyclopedias%20will%20appear%2C%20ready%20made%20with%20a%20mesh%20of%20associative%20trails%20running%20through%20them%2C%20ready%20to%20be%20dropped%20into%20the%20memex%20and%20there%20amplified




This is what connecting people looks like - facebook isnt even scratching the surface


