# Attention is all you have

<!-- toc -->


<!--

The semantic web was an afterthought

maybe the chapter should be called "web3 & the semantic web"

-->


██████████████████████████████████████████████████████████████████
# a
██████████████████████████████████████████████████████████████████

Is it a coincidence that one of the most important papers in AI/ML that introduced transformers was called `"Attention is all you need"`?

One of the most transformative (pun intended) papers in AI/ML is called ...



<!-- > "What information consumes is rather obvious: it consumes the attention of its recipients. Hence a wealth of information creates a poverty of attention, and a need to allocate that attention efficiently among the overabundance of information sources that might consume it." - [Herbert A. Simon](https://www.brainyquote.com/quotes/herbert_a_simon_181919) -->

