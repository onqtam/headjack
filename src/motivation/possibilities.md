<div style="text-align: center;">
    <img src="https://png.pngitem.com/pimgs/s/207-2073499_translate-platform-from-english-to-spanish-work-in.png">
</div>


# A list of what could be possible


imagine stumbling upon a video and saving it for later viewing. Later you decide to check where it got to you from - how do you query that? in a data-centric world you could run a query asking to see if anyone you follow has shared it


hope for spam & moderation: if there is a market for it it could be solved faster as compared when a monopoly has to deal with that - like with spam and email - anyone can build tools to fight spam in email


it doesn't make sense to not be possible to comment on anything in the web and not have your followers see it. simply doesn't

No need to rebuild social graphs in every new interface/platform

"diff" view that summarizes the changes to a URI over time (the updates/edits)
TODO: look at this:
We've got a pretty solid diff view now, notes at https://blog.archive.org/2019/10/18/the-wayback-machine-fighting-digital-extinction-in-new-ways/ and an example at https://web.archive.org/web/diff/20170118202526/20170120040337/https://www.ice.gov/speeches











## Recommendation algorithms

promotion of videos will no longer be dominated just by the recommendation algorithm - when an influencer comments on a video his followers will see it and watch the said video too

We will be able to study the effects of recommendation algorithms for virality because all content has an interface where it came from

Interfaces will compete on the algorithms they let people use to filter and sort information - to look for signal through the noise.

have you seen replies by people you don't follow in your twitter feed? why would "X received a reply from someone you don't follow" be in there? shouldn't there be a setting for that?





















## No more data silos

Generating content should be frictionless. Associating content with identity should be straightforward. Frictionless migration between views

all content should be referencable, sub-quotable, commentable & share-able

We should be able to refer to content uniquely and see the discussion around it

The conversation around a piece of media shouldn't be segregated between Twitter, YouTube & Reddit - it should be 1 viewed through different lenses.

The Twitter view of a discussion is basically the same as just the top level comments for a thread (without showing the children) in Reddit.

segregated and disjoint discussion in the open web serves nobody - there should be canonical IDs for events & information that we can all refer to and discussion can revolve around them

- competing "subreddits"on the same topic with different moderators & content policies

- tagged posts with 1 main tag would simply appear both in a reddit-like ranking and subgroup grouping, and also in a twitter-like platform with default rendering (or optionally showing the tag).

you should be able to quote-comment any piece of science or media and have a threaded conversation with everyone. All publishing platforms will be plugged into this ecosystem.

## Data-centric interoperability & composability

- We'll be able to plot a timeline for all references to an event and filter based on activity & type of references

persistent URIs help with returning back to a resource and checking all the discussion around it, also checking which of those that you follow have mentioned it

- The lack of data interoperability leads to tool & widget incompatibility


## Sub-addressing TODO: change this name

OMG
quoting sub-sections of pages through the internet archive
https://archive.ph/O2D45#selection-635.4-635.18

HOW TO LINK TO THIS CLIP IN A BETTER WAY?!?!?!
`"The internet creates 1 giant aggregator for everything"` - [@naval](https://youtu.be/3qHkcs3kG44?t=3527)
https://youtu.be/3qHkcs3kG44?t=3527
https://youtube.com/clip/UgkxphJhihcVY-U-PLFEvDl1m7Rb-iq4CGgo

clips... out of context - needs to be fixed!

composing videos from clips with narration without losing the original context - movie maker of sorts

imagine a paragraph of a substack article to be retweetable and to be able to go viral while retaining the context of the whole - this is what composability could enable.

TODO: think about memes & the original content + layers

## How limited Twitter is

lists don't show comments & replies that are not to the same person being followed

ability to choose to see someone's reactions/comments or not when added in a list like twitter's lists - different types of levels of filtering for lists

- we can't sort the tweets of someone based on engagement

Thread unrolled view by default

twitter could show the most liked/commented quote tweets to show the reception of a tweet - or we can just look at the comments :|

View as- from index
https://twitter.com/waitbutwhy/status/1502846781150822402
"Treat this follow to a non-list as a list - show me everything they see. Arbitrarily deep in layers." This can be just an interface preference

## Usability improvements & organization

each interface will specialize into providing the best possible service for the particular need/aspect of data consumption


Our minds filter out inconsequential sensations by default but we may tune them in if we so choose to - we should have the same (or greater) level of control in the digital realm. The stream of events for the things we are interested in needs the most sophisticated filtering and configuration possible and anyone should be able to plug into the event bus and develop new tools.




a slider for filtering/jumping through time like in discourse
https://meta.discourse.org/t/change-right-gutter-to-vertical-timeline-topic-controls/44096/231

you can't link to a youtube comment - think about how limiting and yet obvious that is - comments there are just an afterthought



- universal & permanent bookmarks.
bookmarks & something like logseq on top of this global namespace

playlists are lists of bookmarks


we could have one identity, one set of bookmarks that we manage with something like logseq and we can export stories of deduplicated data points and see the conversations around them.

every journalist should be able to use something like logseq and construct their stories with referencable facts - timestamped & crypto provable

Write about the story for a journalist using a tool like that and constructing interlinked stories... that needs a page on its own

quote-retweet something - and then later when viewing the discussion around the original content to be able to view the discussion from quote-retweets 1 level deep as well, or even 2

lowest common denominator UX is fine but the power users should be able to go crazy - let them separate signal from noise on a global level

## Authenticity

imagine being able to point to the first occurrence of a piece of content and being able to prove that there is no prior occurrence (if nobody manages to submit a hash & merkle proof for an earlier block).

imagine how hard it would be to push deepfake clips if the norm for clips is to share the full episode uploaded by a trusted entity with a specific range - we shouldn't accept reuploads of clips out of context

no more tweets with a few screenshots of other tweets - these should be composable & carrying the actual data/references/proofs


we are building our digital history on [shifting sands](https://news.ycombinator.com/item?id=27690525) without a solid foundation.**


It is really tempting to cover for mistakes by pretending they never happened. Our technology now makes that alarmingly simple
https://www.theatlantic.com/technology/archive/2021/06/the-internet-is-a-collective-hallucination/619320/


The public’s interest in seeing what’s changed—or at least being aware that a change has been made and why—is as legitimate as it is diffuse. And because it’s diffuse, few people are naturally in a position to speak on its behalf.
https://www.theatlantic.com/technology/archive/2021/06/the-internet-is-a-collective-hallucination/619320/


https://perma.cc/5ZC2-P4JM
https://perma.cc/JA33-F7F9?type=image
For example, thanks to the site’s record-keeping both of deletions and of the source and text of demands for removals, the law professor Eugene Volokh was able to identify a number of removal requests made with fraudulent documentation—nearly 200 out of 700 “court orders” submitted to Google that he reviewed turned out to have been apparently Photoshopped from whole cloth. The Texas attorney general has since sued a company for routinely submitting these falsified court orders to Google for the purpose of forcing content removals.
https://www.theatlantic.com/technology/archive/2021/06/the-internet-is-a-collective-hallucination/619320/

shifting sands
https://news.ycombinator.com/item?id=27690525

https://twitter.com/balajis/status/1360413999712538627

> Finally, self-authenticating data provides more mechanisms that can be used to establish trust. Self-authenticated data can retain metadata, like who published something and whether it was changed. Reputation and trust-graphs can be constructed on top of users, content, and services. The transparency provided by [verifiable computation](https://en.wikipedia.org/wiki/Verifiable_computing) provides a new tool for establishing trust by showing precisely how the results were produced. We believe verifiable computation will present huge opportunities for sharing indexes and social algorithms without sacrificing trust, but the cryptographic primitives in this field are still being refined and will require active research before they work their way into any products.
https://blueskyweb.xyz/blog/3-6-2022-a-self-authenticating-social-protocol

## Infrastructure improvements

- [Search engines](https://scribe.rip/p/what-every-software-engineer-should-know-about-search-27d1df99f80d):
    - Building indexes would be greatly simplified as they will be plugged to the global message bus and update only on events (push) - instead of periodic batch crawling of the public web (pull). The history of changes will be much more granular, precise, complete, structured & authenticated.
    - Message schemas will greatly aid in the indexability and information extraction from dynamic websites.
    - The move to data-centric addressing and the desegregation of data will lead to a lot less duplicates and more rich & precise context around any event/message.
    - Currently ephemeral experiences such as search suggestions leave no trace and it's extremely hard to prove bias as [Dr. Robert Epstein](https://en.wikipedia.org/wiki/Robert_Epstein#Contributions_to_Internet_Studies) would attest - competition & a lower barrier to entry are direly needed.
    - The range of sophistication of search engines would span the full spectrum - from the data center scale to software that you can run locally at home and most will be specialized - [The Future of Search Is Boutique](https://future.a16z.com/the-future-of-search-is-boutique).

- The [Internet Archive](https://en.wikipedia.org/wiki/Internet_Archive):
    - It will no longer need to actively poll all websites on earth periodically & check for changes and save snapshots - instead it will just watch & save all incoming events and have a complete history without any redundant data & inefficiencies.
    - Actual content & presentation HTML can be decoupled and only the essential could be saved. There could be a new message type for interfaces to signal a change in what they serve to browsers for presentation & rendering of content which the Internet Archive could save throughout time as well to provide the historical views. Redundancy of snapshots can be driven to 0.
    - Content that is no longer accessible through the original interface that published it and is not explicitly archived by the user that posted it would still be accessible by anyone with the same persistent URIs if archival services are queried - even though they won't be directly linked to the interface/user IDs.

- Web infrastructure:
    - dsdfg

    - platforms to deploy interface apps without thinking about the infrastructure
    - SaaS infrastructure companies that host plugin apps and deal with the heavy lifting of data processing
    - Your startup idea cant afford the infrastructure to process 100mb/s of ephemeral data? Pay a service a small fee for a subset of data you’re interested in for your PoC
    - Market for intermediate processed results & indexes so that not everyone needs to reinvent the wheel and build the same set of algorithms and infrastructure over and over again - creating a market for the information pipeline by division of labor & specialization. Any intermediate data structure could be checked for validity based on the inputs - albeit slow. Test but verify - opaque processing rules. No more algorithmic black boxes. Batch processing - web-scale services do that all the time and plenty of work is done on results that are minutes or even hours outdated












- notifications for data references being referenced
    data could be any data/resource/ID
    with filtering
    imagine being able to follow the activity of X person on any possible online venue - the all-encompassing public record
- editors & all forms of media will be able to refer to specific posts uniquely
    - something like #seq!<interface_id/interface_nonce/user_id/content_stuff> which could prompt an index lookup in your editor
        - along with competing indexers
    References are useful beyond peer review - it would be useful for the entire web.
- imagine wikipedia being rebuilt on top of this
    - dead links? thing of the past - can be cached locally & preserved with merkle proofs
    - imagine wikipedia being forked with a different set of moderators - like in git
    - imagine rebuilding wikipedia on top of this graph and being able to reference each paragraph/change
- being able to subscribe to changes of pages in websites that don't have an ID (not through follow (because they don't have an ID) but by other means)
- Imagine the possible visualizations on the graph data of the hivemind
    - imagine a 3d spacial graph of the interconnected knowledge you should learn, and how as you progress you color the edges to the ideas you're studiying on as a progress bar. continue from wherever you want after pausing, explore the datapoints & the interconnectedness. Imagine shipping a 5GB graph in such software for anyone to experience the ideas someone is trying to communicate to them
    - imagine creating explanatory videos for each edge (connections between things in life) - saying how the 2 things interplay, and why
    - imagine wearing a VR headset while constructing your stories as graphs constructed with anchored data
- you'll have the option to subscribe (not on-chain but through an interface) to quotes & references of content that interest you - seeing what the reception is and how that unfolds












wikipedia needs to be git-like. everything can be git-like. new action type: grant ability to someone to "edit" & publish a new version of an item

fork wikipedia?

github needs to be reimplemented on top of this - open source code is a public good - can be interwoven with peer review and public discourse. Any piece of data/event.

- peer review can (and should) be reimplemented on top of infrastructure like this


Imagine having a single id for a film and have comment sections for each chapter or minute


special message tag type: referencing something as a fact - but that thing needs an UUID so ppl can look it up and see what's being commented about it over there
