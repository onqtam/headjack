# Authenticity

Documents are [host-certified](problems.md#the-host-centric-web) and we refer to data by location instead of contents - for more details checkout the [host-centric](../introduction/host_centric.md) page. Let's expand on what [data-centric](../introduction/data_centric.md) addressing with [self-authenticating](https://en.wikipedia.org/wiki/Self-authenticating_document) documents and an open ecosystem where data does not live in silos enables:


<!-- in addition to the host-centric page: -->




imagine being able to point to the first occurrence of a piece of content and being able to prove that there is no prior occurrence (if nobody manages to submit a hash & merkle proof for an earlier block).

imagine how hard it would be to push deepfake clips if the norm for clips is to share the full episode uploaded by a trusted entity with a specific range - we shouldn't accept reuploads of clips out of context

no more tweets with a few screenshots of other tweets - these should be composable & carrying the actual data/references/proofs



we are building our digital history on [shifting sands](https://news.ycombinator.com/item?id=27690525) without a solid foundation.**


> "It is really tempting to cover for mistakes by pretending they never happened. Our technology now makes that alarmingly simple" - [source](https://www.theatlantic.com/technology/archive/2021/06/the-internet-is-a-collective-hallucination/619320/)




> Society can’t understand itself if it can’t be honest with itself, and it can’t be honest with itself if it can only live in the present moment. It’s long overdue to affirm and enact the policies and technologies that will let us see where we’ve been, including and especially where we’ve erred, so we might have a coherent sense of where we are and where we want to go.
https://www.theatlantic.com/technology/archive/2021/06/the-internet-is-a-collective-hallucination/619320/




<!-- The public’s interest in seeing what’s changed—or at least being aware that a change has been made and why—is as legitimate as it is diffuse. And because it’s diffuse, few people are naturally in a position to speak on its behalf.
https://www.theatlantic.com/technology/archive/2021/06/the-internet-is-a-collective-hallucination/619320/ -->

lets try to achieve consensus on ground truth with the ledger of record
https://en.wikipedia.org/wiki/Ground_truth


https://perma.cc/5ZC2-P4JM
https://perma.cc/JA33-F7F9?type=image
For example, thanks to the site’s record-keeping both of deletions and of the source and text of demands for removals, the law professor Eugene Volokh was able to identify a number of removal requests made with fraudulent documentation—nearly 200 out of 700 “court orders” submitted to Google that he reviewed turned out to have been apparently Photoshopped from whole cloth. The Texas attorney general has since sued a company for routinely submitting these falsified court orders to Google for the purpose of forcing content removals.
https://www.theatlantic.com/technology/archive/2021/06/the-internet-is-a-collective-hallucination/619320/

https://twitter.com/balajis/status/1360413999712538627

> Finally, self-authenticating data provides more mechanisms that can be used to establish trust. Self-authenticated data can retain metadata, like who published something and whether it was changed. Reputation and trust-graphs can be constructed on top of users, content, and services. The transparency provided by [verifiable computation](https://en.wikipedia.org/wiki/Verifiable_computing) provides a new tool for establishing trust by showing precisely how the results were produced. We believe verifiable computation will present huge opportunities for sharing indexes and social algorithms without sacrificing trust, but the cryptographic primitives in this field are still being refined and will require active research before they work their way into any products.
https://blueskyweb.xyz/blog/3-6-2022-a-self-authenticating-social-protocol






web of trust






